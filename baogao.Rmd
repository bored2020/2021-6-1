---
title: "收缩方法相关内容"
author: "冯裕祺"
date: "2021/6/1"
output: 
  beamer_presentation: 
    latex_engine: xelatex
    toc: yes
    theme: Berkeley
    colortheme: dolphin
    slide_level: 2
    fonttheme: structureitalicserif
    highlight: espresso
fontsize: 15pt
header-includes:
- \usepackage{ctex}
---

# 收缩的方法

通过保留一部分预测变量而丢弃剩余的变量，*子集选择 (subset selection)* 可得到一个可解释的、预测误差可能比全模型低的模型．然而，因为这是一个离散的过程（变量不是保留就是丢弃），所以经常表现为高方差，因此不会降低全模型的预测误差．而*收缩方法 (shrinkage methods)* 更加连续，因此不会受高易变性 (high variability) 太大的影响。


# 岭回归

岭回归 (Ridge regression) 根据回归系数的大小加上惩罚因子对它们进行收缩．岭回归的系数使得带惩罚的残差平方和最小：
$$
\hat{\beta}^{ridge}=\underset{\beta}{\arg\min}\Big\{\sum\limits_{i=1}^N(y_i-\beta_0-\sum\limits_{j=1}^px_{ij}\beta_j)^2+\lambda\sum\limits_{j=1}^p\beta_j^2\Big\}
\tag{1}
\label{1}
$$
这里$\lambda\ge0$是控制收缩程度的参数：$\lambda$值越大，收缩的程度越大．每个系数都向零收缩．<!--系数向零收缩（并且彼此收缩到一起）．-->通过参数的平方和来惩罚的想法也用在了神经网络，也被称作 **权重衰减 (weight decay)**


---

岭回归问题可以等价地写成：
$$
\begin{aligned}
\hat{\beta}^{\text {ridge }}=& \arg \min \sum_{\beta}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2} \\
\text { subject to } \sum_{j=1}^{p} \beta_{j}^{2} \leq t
\end{aligned}
\tag{2}
\label{2}
$$
上式用参数显式表达了对回归参数大小的约束。式 \eqref{2} 其实是对式 \eqref{1} 应用 Lagrange 乘子法得到的。


---

\eqref{1} 中的 $\lambda$ 和 \eqref{2} 中的 $t$ 存在一一对应．当在线性回归模型中有许多相关变量，它们的系数可能很难确定且有高方差．某个变量的较大的正系数可以与相关性强的变量的差不多大的负系数相互抵消．通过对系数加入大小限制，如 \eqref{2}，这个问题能得以减轻．


---

  这里说的是，在没有对参数大小进行限制前，会存在一对相关性强的变量，它们系数取值符号相反，但绝对值差不多大，会大大增加方差，这也就是高方差的体现，但其实它们的合作用效果近似为 $0$，所以考虑引进对参数大小的惩罚。

对输入按比例进行缩放时，岭回归的解不相等，因此求解 \eqref{1} 前我们需要对输入进行标准化．另外，注意到惩罚项不包含截距 $\beta_0$．对截距的惩罚会使得过程依赖于 $\mathbf{Y}$ 的初始选择；也就是，对每个 $y_i$ 加上常数 $c$ 不是简单地导致预测值会偏离同样的量 $c$．可以证明经过对输入进行中心化（每个 $x_{ij}$ 替换为 $x_{ij}-\bar x_j$）后，\eqref{1} 的解可以分成两部分．我们用 $\bar y=\frac{1}{N}\sum_1^Ny_i$ 来估计 $\beta_0$．剩余的参数利用中心化的 $x_{ij}$ 通过无截距的岭回归来估计．今后我们假设中心化已经完成，则输入矩阵 $\mathbf X$ 有 $p$（不是 $p+1$）列．


--- 

将\eqref{1}的准则写成矩阵的形式：
$$\operatorname{RSS}(\lambda)=(\mathbf{y}-\mathbf{X} \beta)^{T}(\mathbf{y}-\mathbf{X} \beta)+\lambda \beta^{T} \beta \tag{3}
\label{3}
$$
可以看出岭回归的解为:
$$\hat{\beta}^{\text {ridge }}=\left(\mathbf{X}^{\mathbf{T}} \mathbf{X}+\lambda \mathbf{I}\right)^{-1} \mathbf{X}^{T} \mathbf{y} 
\tag{4}
\label{4}
$$

其中 $\mathbf{I}$ 为 $p\times p$ 的单位矩阵．注意到选择二次函数惩罚 $\beta^T\beta$，岭回归的解仍是 $\mathbf{y}$ 的线性函数．解在求逆之前向矩阵 $\mathbf{X^TX}$ 的对角元上加入正的常数值．即使 $\mathbf{X^TX}$ 不是满秩，这样会使得问题非奇异，而且这是第一次将岭回归引入统计学中 (Hoerl and Kennard, 1970）的主要动力。传统的岭回归的描述从定义 \eqref{4} 开始．我们选择通过 \eqref{1} 和 \eqref{2} 来阐述，因为这两式让我们看清楚了它是怎样实现的。


# 岭回归的*Bayes*角度

当给定一个合适的先验分布，岭回归也可以从后验分布的均值或众数得到．具体地，假设 $y_i \sim N(\beta_0+x^T_i\beta,\sigma^2)$，参数 $\beta_j$ 的分布均为 $N(0,\tau^2)$，每个都相互独立．则当 $\tau^2$ 和 $\sigma^2$ 值已知时，$\beta$ 后验分布密度函数的对数值（的负数）与 \eqref{1} 中花括号里面的表达式成比例 ，且 $\lambda=\sigma^2/\tau^2$。因此岭回归估计是后验分布的众数；又因分布为高斯分布，则也是后验分布的均值。**正态分布均值中位数众数相等**


# 从*奇异值分解*角度看岭回归

中心化输入矩阵 $\mathbf{X}$ 的 **奇异值分解 (SVD)** 让我们进一步了解了岭回归的本质．这个分解在许多统计方法分析中非常有用．$N\times p$ 阶矩阵 $\mathbf{X}$ 的 SVD 分解有如下形式：

$$\mathbf{X=UDV^T}
\tag{5}
\label{5}
$$
这里 $\mathbf{U}$ 和 $\mathbf{V}$ 分别是 $N\times p$ 和 $p\times p$ 的正交矩阵，$\mathbf{U}$的列张成 $X$ 的列空间，$\mathbf{V}$ 的列张成 $X$ 的行空间．$\mathbf{D}$ 为 $p\times p$ 的对角矩阵，对角元 $d_1\ge d_2 \ge \cdots \ge d_p \ge 0$ 称作 $\mathbf{X}$ 的奇异值．如果一个或多个 $d_j=0$，则 $\mathbf{X}$ 为奇异的．


---

利用奇异值分解，通过化简我们可以把最小二乘拟合向量写成：
$$\begin{aligned}
\mathbf{X} \hat{\beta}^{l s} &=\mathbf{X}\left(\mathbf{X}^{\mathbf{T}} \mathbf{X}\right)^{-\mathbf{1}} \mathbf{X}^{\mathbf{T}} \mathbf{y} \\
&=\mathbf{U U}^{\mathbf{T}} \mathbf{y}
\end{aligned}$$
注意到 $\mathbf{U}^T\mathbf y$ 是 $\mathbf{y}$ 正交基 $\mathbf{U}$ 下的坐标．同时注意其与 \eqref{3} 的相似性；
$$
\begin{array}{l}
\hat{\beta}=\mathbf{R}^{-1} \mathbf{Q}^{\mathbf{T}} \mathbf{y} \\
\hat{\mathbf{y}}=\mathbf{Q Q}^{\mathbf{T}} \mathbf{y}
\end{array}
$$
$\mathbf{Q}$ 和 $\mathbf{U}$ 是 $\mathbf{X}$ 列空间的两个不同的正交基。


---

现在岭回归的解为：
$$\begin{aligned}
\mathbf{X}\hat{\beta}^{ridge}&=\mathbf{X}(\mathbf{X^TX}+\lambda \mathbf{I})^{-1}\mathbf{X^Ty}\notag\\
&= \mathbf{UD}(\mathbf{D^2}+\lambda \mathbf{I})^{-1}\mathbf{DU^Ty}\notag\\
&= \sum\limits_{j=1}^p\mathbf{u}_j\dfrac{d_j^2}{d_j^2+\lambda}\mathbf{u_j^Ty}
\end{aligned}
\tag{7}
$$
其中 $\mathbf{u}_j$ 是 $\mathbf{U}$ 的列向量．注意到因为 $\lambda \ge 0$，我们有 $d_j^2/(d^2_j+\lambda)\le 1$．类似线性回归，岭回归计算 $\mathbf{y}$ 关于正规基 $\mathbf{U}$ 的坐标．通过因子 $d^2_j/(d^2_j+\lambda)$ 来收缩这些坐标．这意味着更小的 $d_j^2$ 会在更大程度上收缩基向量的坐标．

---

$d_j^2$ 值小意味着什么？中心化后的矩阵 $\mathbf{X}$ 的奇异值分解是表示 $\mathbf{X}$ 中主成分变量的另一种方式．样本协方差矩阵为 $\mathbf{S=X^TX}/N$<!--$\mathbf{S={\color{red} E((X-EX)^T(X-EX))=}X^TX}/N$-->，并且从 \eqref{5} 式我们得到
$$
\mathbf{X^T X = VD^2V^T}\tag{8}
$$
上式是 $\mathbf{X^TX}$（当忽略因子 $N$ 时，也是 $S$）的 **特征值分解 (eigen decomposition)**．特征向量 $v_j$（$\mathbf{V}$ 的列向量）也称作 $\mathbf{X}$ 的 **主成分 (principal components)**（或 Karhunen-Loeve）方向．第一主成分方向 $v_1$ 有下面性质：$\mathbf{z}_1=\mathbf{X}v_1$ 在所有 $\mathbf{X}$ 列的标准化线性组合中有最大的样本方差．样本方差很容易看出来是
$$
\operatorname{Var}\left(\mathbf{z}_{1}\right)=\operatorname{Var}\left(\mathbf{X} v_{1}\right)=\frac{d_{1}^{2}}{N}
\tag{9}
$$

---


事实上 $\mathbf{z}_1=\mathbf{X}v_1=\mathbf{u}_1d_1$．导出变量 $\mathbf{z_1}$ 称作 $\mathbf{X}$ 的第一主成分，因此 $\mathbf{u_1}$ 是标准化的第一主成分．后面的主成分 $z_j$ 在与前一个保持正交的前提下有最大的方差 $d_j^2/N$．所以，最后一个主成分有最小的方差．因此越小的奇异值 $d_j$ 对应 $\mathbf{X}$ 列空间中方差越小的方向，并且岭回归在这些方向上收缩得最厉害。


--- 

```{r,fig.asp=0.5, fig.height=7, fig.width=4,echo=FALSE}
knitr::include_graphics("C:/Users/YuQiFeng/Desktop/acadmeic files/2021-6-1/2021-6-1/fig1.png")
```










































--- 


我们定义有效自由度为：
$$
\begin{aligned}
\operatorname{df}(\lambda) &=\operatorname{tr}\left[\mathbf{X}\left(\mathbf{X}^{\mathbf{T}} \mathbf{X}+\lambda \mathbf{I}\right)^{-1} \mathbf{X}^{T}\right] \\
&=\operatorname{tr}\left(\mathbf{H}_{\lambda}\right) \\
&=\sum_{j=1}^{p} \frac{d_{j}^{2}}{d_{j}^{2}+\lambda}
\end{aligned}
\tag{10}
$$
上面 $\lambda$ 的单调递减函数是岭回归拟合的 **有效自由度 (effective degrees of freedom)**．通常在含 $p$ 个变量的线性回归拟合中，拟合的自由度为 $p$，也就是无约束参数的个数．这里想法是尽管岭回归拟合中所有的 $p$ 个系数都不为 0，但是它们在由 $\lambda$ 控制的约束下拟合．注意到当 $\lambda=0$（没有正则化）时 $df(\lambda)=p$，并且当 $\lambda\rightarrow \infty$ 时 $df(\lambda)\rightarrow 0$．当然总是对于截距总有一个额外的自由度，事先 (apriori) 已经去掉了。


# *LASSO*

$$
\begin{aligned}
\hat{\beta}^{lasso}&=\underset{\beta}{\arg\min}\sum\limits_{i=1}^N(y_i-\beta_0-\sum\limits_{j=1}^px_{ij}\beta_j)^2\notag\\
&\text{subject to }\sum\limits_{j=1}^p\vert\beta_j\vert\le t 
\end{aligned}
\tag{11}
$$
我们也可以把`lasso`写成拉格朗日形式。
$$\hat{\beta}^{\text {lasso }}=\underset{\beta}{\arg \min }\left\{\sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}+\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|\right\}
$$




#  讨论比较三种方法

在正交输入矩阵的情况下，三种过程都有显式解．每种方法对最小二乘估计 $\hat{\beta}_j$ 应用简单的变换，详见下表。


---

```{r,fig.asp=0.5, fig.height=5, fig.width=4,echo=FALSE,fig.align='center'}
knitr::include_graphics("C:/Users/YuQiFeng/Desktop/acadmeic files/2021-6-1/2021-6-1/fig2.png")
```



---

岭回归做等比例的收缩．lasso 通过常数因子 $\lambda$ 变换每个系数，在 0 处截去．这也称作“软阈限”，而且用在 基于小波光滑的内容中．最优子集选择删掉所有系数小于第 $M$ 个大系数的变量；这是“硬阈限”的一种形式．



--- 

回到非正交的情形，一些图象可以帮助了解它们之间的关系．当只有两个参数时图 3.11 描绘了 lasso（左）和岭回归（右）．残差平方和为椭圆形的等高线，以全最小二乘估计为中心．岭回归的约束区域为圆盘 $\beta_1^2+\beta_2^2\le t$，lasso 的约束区域为菱形$\vert\beta_1\vert+\vert\beta_2\vert\le t$．两种方式都寻找当椭圆等高线达到约束区域的第一个点．与圆盘不同，**菱形 (diamond)** 有角；如果解出现在角上，则有一个参数 $\beta_j$ 等于 0．当 $p > 2$，菱形变成了 **偏菱形 (rhomboid)**，而且有许多角，平坦的边和面；对于参数估计有更多的可能为 0．


---

图 3.11 lasso (左)和岭回归（右）的估计图象．图中显示了误差的等高线和约束函数．实心蓝色区域分别为约束区域$\vert\beta_1\vert+\vert\beta_2\vert\le t$以及$\beta^2_1+\beta_2^2\le t^2$，红色椭圆为最小二乘误差函数的等高线．

```{r echo=FALSE, fig.align='center', fig.height=2, fig.width=1}
knitr::include_graphics("C:/Users/YuQiFeng/Desktop/acadmeic files/2021-6-1/2021-6-1/fig3.png")
```



---

我们可以把岭回归和 lasso 一般化，并且可以看成是贝叶斯估计．考虑下面准则
$$
\tilde{\beta}=\underset{\beta}{\arg\min}\Big\{\sum\limits_{i=1}^N(y_i-\beta_0-\sum\limits_{j=1}^px_{ij}\beta_j)^2+\lambda\sum\limits_{j=1}^p\vert\beta_j\vert^q\Big\}
\tag{13}
$$


---

```{r echo=FALSE, fig.align='center', fig.height=2, fig.width=1}
knitr::include_graphics("C:/Users/YuQiFeng/Desktop/acadmeic files/2021-6-1/2021-6-1/fig4.png")
```


--- 

将 $\vert\beta_j\vert^q$ 看成 $\beta_j$ 的先验概率密度的对数值，同样有参数先验分布的等高线．$q=0$ 对应变量子集选择，惩罚项是简单地统计非零参数的个数；$q=1$ 对应 lasso，$q=2$ 对应岭回归．注意到 $q\le 1$，先验在各方向上不是均匀的，而是更多地集中在坐标方向上．对应 $q=1$ 情形的先验分布是关于每个输入变量是的独立的二重指数分布（或者 Laplace 分布），概率密度为$(1/2\tau)exp(-\vert\beta\vert)/\tau$ 并且 $\tau=1/\lambda$．$q=1$ 的情形（lasso）是使得约束区域为凸的最小 $q$ 值；非凸约束区域使得优化问题很困难．

从这点看，lasso、岭回归和最优子集选择是有着不同先验分布的贝叶斯估计．然而，注意到它们取自后验分布的众数，即最大化后验分布．在贝叶斯估计中使用后验分布的均值更加常见．岭回归同样是后验分布的均值，但是 lasso 和最优子集选择不是．


--- 

再一次观察准则 \eqref{13}，我们可能尝试除 0，1，2 外的其它 $q$ 值．尽管有人可能从数据中估计 $q$，我们的经验表明引入额外的方差不值得．$q\in (1,2)$ 表明在 lasso 和岭回归之间进行权衡．当 $ q > 1$ 时尽管 $\vert\beta_j\vert^q$ 在 0 处可导，但是并没有lasso（$q=1$）的令系数恰巧为零的性质．部分由于这个原因并且考虑计算易处理，Zou and Hastie (2005)[4] 引入弹性惩罚
$$
\lambda \sum\limits_{j=1}^p(\alpha\beta_j^2+(1-\alpha)\vert\beta_j\vert)\tag{14}
$$
这是一种岭回归和 lasso之间的平衡．图 3.13 比较了 $q=1.2$ 下的 $L_q$ 惩罚以及 $\alpha=0.2$ 的弹性网惩罚；很难从肉眼来观察出差异．弹性网像 lasso 一样选择变量，同时像岭回归一样收缩相关变量的系数．同时考虑了 $L_q$ 惩罚的计算优势．

---

```{r echo=FALSE, fig.align='center', fig.height=2, fig.width=1}
knitr::include_graphics("C:/Users/YuQiFeng/Desktop/acadmeic files/2021-6-1/2021-6-1/fig5.png")
```


--- 

图3.13 $q=1.2$ 时 $\sum_j\vert\beta_j\vert^q$ 为常数值的轮廓线（左图）以及 $\alpha=0.2$ 时弹性网惩罚 $\sum_j(\alpha\beta_j^2+(1-\alpha)\vert\beta_j\vert)$ 为常数值的轮廓线（右图）．尽管看起来很相似，弹性网有尖角（不可导），而 $q=1.2$ 的惩罚不会有尖角．




















